{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9333422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit: Most of these codes are from the NLP course by Deeplearn.AI on Coursera\n",
    "#I created the text generating part myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b58738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd76f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir(r\"C:\\Users\\User\\Desktop\\python\\prospectus\\files\\\\\")\n",
    "data = \"\"\n",
    "for file in file_names:\n",
    "    file_name = r\"C:\\Users\\User\\Desktop\\python\\prospectus\\files\\\\\" + file\n",
    "    reader = PdfReader(file_name)    \n",
    "    for i in range(len(reader.pages)):\n",
    "        page = reader.pages[i]\n",
    "        data += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb62063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    sentences = data.replace(\"\\n\", \" \").split(\". \")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dda697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        tokenized_processed = [t.lower() for t in tokenized if not t.isupper()]\n",
    "        if len(tokenized_processed) > 5:\n",
    "            tokenized_sentences.append(tokenized_processed)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "197b8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    return tokenize_sentences(split_to_sentences(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b290226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            word_counts[token] = word_counts.get(token, 0) + 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e97b4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "477130e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, \n",
    "                             vocabulary, \n",
    "                             unknown_token=\"<unk>\"):\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cebf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, \n",
    "                    count_threshold, \n",
    "                    unknown_token=\"<unk>\", \n",
    "                    get_words_with_nplus_frequency=get_words_with_nplus_frequency, \n",
    "                    replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
    "    vocabulary = get_words_with_nplus_frequency(data, count_threshold)\n",
    "    data_replaced = replace_oov_words_by_unk(data, vocabulary, unknown_token)\n",
    "    return data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba1b6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token=\"<s>\", end_token=\"<e>\"):\n",
    "    n_grams = {}\n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        for i in range(len(sentence) - (n - 1)):\n",
    "            n_gram = sentence[i:i+n]\n",
    "            n_grams[n_gram] = n_grams.get(n_gram, 0) + 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e810e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word,\n",
    "                         previous_n_gram,\n",
    "                        n_gram_counts,\n",
    "                        n_plus1_gram_counts,\n",
    "                        vocabulary_size, \n",
    "                        k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    n_plus1_gram = previous_n_gram + (word, )\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a68c1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram,\n",
    "                          n_gram_counts,\n",
    "                          n_plus1_gram_counts,\n",
    "                          vocabulary,\n",
    "                          end_token=\"<e>\",\n",
    "                           unknown_token=\"<unk>\",\n",
    "                           k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    vocabulary = vocabulary + [end_token, unknown_token]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probabilities[word] = estimate_probability(word,\n",
    "                                                   previous_n_gram,\n",
    "                                                   n_gram_counts, \n",
    "                                                   n_plus1_gram_counts, \n",
    "                                                   vocabulary_size, \n",
    "                                                   k=k)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27d8a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens,\n",
    "                  n_gram_counts,\n",
    "                  n_plus1_gram_counts,\n",
    "                  vocabulary,\n",
    "                  end_token=\"<e>\",\n",
    "                  unknown_token=\"<unk>\",\n",
    "                   k=1.0,\n",
    "                   start_with=None):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_tokens = [\"<s>\"] * n + previous_tokens\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    if tuple(previous_n_gram) not in n_gram_counts:\n",
    "        return None\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                          n_gram_counts,\n",
    "                                          n_plus1_gram_counts,\n",
    "                                          vocabulary,\n",
    "                                          k=k)\n",
    "    \n",
    "    del probabilities[\"<unk>\"]\n",
    "    \n",
    "    probabilities_sorted = {k: v for k, v in sorted(probabilities.items(), key=lambda item: item[1], reverse=True)}\n",
    "        \n",
    "    prob_sorted_top = []\n",
    "    max_prob = list(probabilities_sorted.items())[0][1]\n",
    "    for item in list(probabilities_sorted.items()):\n",
    "        if item[1] == max_prob:\n",
    "            prob_sorted_top.append(item)\n",
    "        \n",
    "    words = [t[0] for t in prob_sorted_top]\n",
    "    probs = [t[1] for t in prob_sorted_top]\n",
    "    \n",
    "    return random.choices(words, weights=probs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b81961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens,\n",
    "                   n_gram_counts_list,\n",
    "                   vocabulary,\n",
    "                    k=1.0,\n",
    "                    start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts - 1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i + 1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, \n",
    "                                    n_gram_counts,\n",
    "                                    n_plus1_gram_counts, \n",
    "                                    vocabulary,\n",
    "                                    k=k, \n",
    "                                    start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91a45072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17793\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "print(len(tokenized_data))\n",
    "\n",
    "minimum_freq = 2\n",
    "data_processed, vocabulary = preprocess_data(tokenized_data, minimum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c26830d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0908d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['business'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'flow', 'flow', 'model']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"business\"\n",
    "previous_tokens = text.split(\" \")\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f997325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have also implemented a number of internal rules and policies to extend the lifecycle of our games ’ ’ .  we have also entered into collaboration and license agreements with 21,123 ) ( 6.4 ) ( 34,354 ) ( 3.5 ) ( ofthis document .  we have also implemented stringent control on our inventory level and the requirements of our customers .  we have also adopted treasury policies to manage our daily expenses and cash withdrawals , so as to ensure our working capital sufficiency by taking advantage of modern technologies to develop servicecategory .  we have also established  "
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "previous_tokens = []\n",
    "output = \"\"\n",
    "for i in range(100):\n",
    "    \n",
    "    candidates = get_suggestions(previous_tokens,\n",
    "                                 n_gram_counts_list,\n",
    "                                 vocabulary,\n",
    "                                 k=1.0,\n",
    "                                 start_with=None)\n",
    "    \n",
    "    for j in range(N):\n",
    "        if candidates[N - j - 1] != None:\n",
    "            next_word = candidates[N - j - 1]\n",
    "            break\n",
    "    \n",
    "    previous_tokens.append(next_word)\n",
    "    output += next_word + \" \"\n",
    "    if (next_word == \"<e>\") or (i==99):\n",
    "        print(output.replace(\"<e>\", \".\"), end=\" \")\n",
    "        previous_tokens = []\n",
    "        output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b303d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
